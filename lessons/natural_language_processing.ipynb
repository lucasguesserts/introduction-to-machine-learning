{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import urllib\n",
    "import string\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "if not os.path.isfile(\"datasets/mini.h5\"):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = \"http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5\"\n",
    "    urllib.request.urlretrieve(conceptnet_url, \"datasets/mini.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 362891\n",
      "all_embeddings dimensions: (362891, 300)\n",
      "Random example word: /c/en/brainwash\n"
     ]
    }
   ],
   "source": [
    "# Load the file and pull out words and embeddings\n",
    "with h5py.File(\"datasets/mini.h5\", \"r\") as f:\n",
    "    all_words = [word.decode(\"utf-8\") for word in f[\"mat\"][\"axis1\"][:]]\n",
    "    all_embeddings = f[\"mat\"][\"block0_values\"][:]\n",
    "\n",
    "print(f\"all_words dimensions: {len(all_words)}\")\n",
    "print(f\"all_embeddings dimensions: {all_embeddings.shape}\")\n",
    "\n",
    "print(f\"Random example word: {all_words[36289]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English words in all_words: 150875\n",
      "english_embeddings dimensions: (150875, 300)\n",
      "random word: activated_carbon\n"
     ]
    }
   ],
   "source": [
    "# Restrict our vocabulary to just the English words\n",
    "english_words = []\n",
    "english_embeddings = []\n",
    "for i in range(len(all_words)):\n",
    "    word = all_words[i]\n",
    "    word_embedding = all_embeddings[i]\n",
    "    if word.startswith(\"/c/en/\"):\n",
    "        english_words.append(word[6:])\n",
    "        english_embeddings.append(word_embedding)\n",
    "english_embeddings = np.array(english_embeddings)\n",
    "\n",
    "print(f\"Number of English words in all_words: {len(english_words)}\")\n",
    "print(f\"english_embeddings dimensions: {english_embeddings.shape}\")\n",
    "\n",
    "print(f\"random word: {english_words[1337]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nomalize embeddings, make all of them have norm equals to 1\n",
    "norms = np.linalg.norm(english_embeddings, axis=1)\n",
    "normalized_embeddings = english_embeddings.astype(\"float32\") / norms.astype(\n",
    "    \"float32\"\n",
    ").reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast lookup\n",
    "index = {word: i for i, word in enumerate(english_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat        ~ cat        = +1.00000\n",
      "cat        ~ feline     = +0.81995\n",
      "cat        ~ dog        = +0.59072\n",
      "cat        ~ moo        = +0.00395\n",
      "cat        ~ freeze     = -0.03023\n",
      "antonym    ~ opposite   = +0.39411\n",
      "antonym    ~ synonym    = +0.46884\n"
     ]
    }
   ],
   "source": [
    "# measure the similarity between words\n",
    "def similarity_score(w1, w2):\n",
    "    c1 = normalized_embeddings[index[w1], :]\n",
    "    c2 = normalized_embeddings[index[w2], :]\n",
    "    score = np.dot(c1, c2)\n",
    "    return score\n",
    "\n",
    "\n",
    "def print_similarity(w1, w2):\n",
    "    print(f\"{w1:10s} ~ {w2:10s} = {similarity_score(w1, w2):+.5f}\")\n",
    "\n",
    "\n",
    "# A word is as similar with itself as possible:\n",
    "print_similarity(\"cat\", \"cat\")\n",
    "\n",
    "# Closely related words still get high scores:\n",
    "print_similarity(\"cat\", \"feline\")\n",
    "print_similarity(\"cat\", \"dog\")\n",
    "\n",
    "# Unrelated words, not so much\n",
    "print_similarity(\"cat\", \"moo\")\n",
    "print_similarity(\"cat\", \"freeze\")\n",
    "\n",
    "# Antonyms are still considered related, sometimes more so than synonyms\n",
    "print_similarity(\"antonym\", \"opposite\")\n",
    "print_similarity(\"antonym\", \"synonym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'humane_society', 'kitten', 'feline', 'colocolo', 'cats', 'kitty', 'maine_coon', 'housecat', 'sharp_teeth']\n",
      "['dog', 'dogs', 'wire_haired_dachshund', 'doggy_paddle', 'lhasa_apso', 'good_friend', 'puppy_dog', 'bichon_frise', 'woof_woof', 'golden_retrievers']\n",
      "['duke', 'dukes', 'duchess', 'duchesses', 'ducal', 'dukedom', 'duchy', 'voivode', 'princes', 'prince']\n"
     ]
    }
   ],
   "source": [
    "def closest_to_vector(v, n):\n",
    "    all_scores = np.dot(normalized_embeddings, v)\n",
    "    best_words = list(map(lambda i: english_words[i], reversed(np.argsort(all_scores))))\n",
    "    return best_words[:n]\n",
    "\n",
    "\n",
    "def most_similar(w, n):\n",
    "    return closest_to_vector(normalized_embeddings[index[w], :], n)\n",
    "\n",
    "\n",
    "print(most_similar(\"cat\", 10))\n",
    "print(most_similar(\"dog\", 10))\n",
    "print(most_similar(\"duke\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nephew']\n",
      "['wife']\n",
      "['paris']\n"
     ]
    }
   ],
   "source": [
    "def solve_analogy(a1, b1, a2):\n",
    "    b2 = (\n",
    "        normalized_embeddings[index[b1], :]\n",
    "        - normalized_embeddings[index[a1], :]\n",
    "        + normalized_embeddings[index[a2], :]\n",
    "    )\n",
    "    return closest_to_vector(b2, 1)\n",
    "\n",
    "\n",
    "print(solve_analogy(\"grandmother\", \"niece\", \"grandfather\"))\n",
    "print(solve_analogy(\"man\", \"husband\", \"woman\"))\n",
    "print(solve_analogy(\"spain\", \"madrid\", \"france\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings in deep models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of inputs: (1411, 300)\n",
      "Shape of labels: (1411, 1)\n"
     ]
    }
   ],
   "source": [
    "# download data\n",
    "movie_simple_file_path = \"datasets/movie-simple.txt\"\n",
    "\n",
    "if not os.path.isfile(movie_simple_file_path):\n",
    "    print(\"Downloading movie-simple.txt...\")\n",
    "    movie_simple_url = \"https://raw.githubusercontent.com/duke-mlss/Duke-MLSS-2018/master/movie-simple.txt\"\n",
    "    urllib.request.urlretrieve(movie_simple_url, movie_simple_file_path)\n",
    "\n",
    "\n",
    "remove_punct = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "\n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "\n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words if w in index]\n",
    "\n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "xs = []\n",
    "ys = []\n",
    "with open(movie_simple_file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "    for l in f.readlines():\n",
    "        x, y = convert_line_to_example(l)\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "# Concatenate all examples into a numpy array\n",
    "xs = np.vstack(xs)\n",
    "ys = np.vstack(ys)\n",
    "\n",
    "# shuffle\n",
    "shuffle_idx = np.random.permutation(xs.shape[0])\n",
    "xs = xs[shuffle_idx, :]\n",
    "ys = ys[shuffle_idx, :]\n",
    "\n",
    "# log\n",
    "print(\"Shape of inputs: {}\".format(xs.shape))\n",
    "print(\"Shape of labels: {}\".format(ys.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = xs.shape[0]\n",
    "\n",
    "num_train = math.floor(0.8 * num_examples)\n",
    "\n",
    "x_train = torch.tensor(xs[:num_train])\n",
    "y_train = torch.tensor(ys[:num_train], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor(xs[num_train:])\n",
    "y_test = torch.tensor(ys[num_train:], dtype=torch.float32)\n",
    "\n",
    "reviews_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "reviews_test = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(reviews_train, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(reviews_test, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Train Loss: 0.6855358481407166 \t Train Acc: 0.5460993051528931\n",
      "Epoch: 25 \t Train Loss: 0.18884149193763733 \t Train Acc: 0.9521276354789734\n",
      "Epoch: 50 \t Train Loss: 0.1113380566239357 \t Train Acc: 0.9689716100692749\n",
      "Epoch: 75 \t Train Loss: 0.08995165675878525 \t Train Acc: 0.9751772880554199\n",
      "Epoch: 100 \t Train Loss: 0.05221224203705788 \t Train Acc: 0.9796099066734314\n",
      "Epoch: 125 \t Train Loss: 0.019935395568609238 \t Train Acc: 0.9858155846595764\n",
      "Epoch: 150 \t Train Loss: 0.008389944210648537 \t Train Acc: 0.9867021441459656\n",
      "Epoch: 175 \t Train Loss: 0.05249840021133423 \t Train Acc: 0.9902482032775879\n",
      "Epoch: 200 \t Train Loss: 0.01095837913453579 \t Train Acc: 0.9946808218955994\n",
      "Epoch: 225 \t Train Loss: 0.033099930733442307 \t Train Acc: 0.9982269406318665\n",
      "Test accuracy: 0.9434629082679749\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "class SWEM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(300, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "## Training\n",
    "# Instantiate model\n",
    "model = SWEM()\n",
    "\n",
    "# Binary cross-entropy (BCE) Loss and Adam Optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Iterate through train set minibatchs \n",
    "for epoch in range(250):\n",
    "    correct = 0\n",
    "    num_examples = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero out the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        loss = criterion(y, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(y))\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_examples += len(inputs)\n",
    "    \n",
    "    # Print training progress\n",
    "    if epoch % 25 == 0:\n",
    "        acc = correct/num_examples\n",
    "        print(\"Epoch: {0} \\t Train Loss: {1} \\t Train Acc: {2}\".format(epoch, loss, acc))\n",
    "\n",
    "## Testing\n",
    "correct = 0\n",
    "num_test = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iterate through test set minibatchs \n",
    "    for inputs, labels in test_loader:\n",
    "        # Forward pass\n",
    "        y = model(inputs)\n",
    "        \n",
    "        predictions = torch.round(torch.sigmoid(y))\n",
    "        correct += torch.sum((predictions == labels).float())\n",
    "        num_test += len(inputs)\n",
    "    \n",
    "print('Test accuracy: {}'.format(correct/num_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of the word 'interesting': 1\n",
      "Sentiment of the word 'brokeback': 1\n",
      "Sentiment of the word 'bad': 0\n",
      "Sentiment of the word 'what': 1\n"
     ]
    }
   ],
   "source": [
    "# Check some words\n",
    "words_to_test = [\"interesting\", \"brokeback\", \"bad\", \"what\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    x = torch.tensor(normalized_embeddings[index[word]].reshape(1, 300))\n",
    "    print(\"Sentiment of the word '{0}': {1}\".format(word, round(float(torch.sigmoid(model(x))[0][0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the embedding weights: torch.Size([5000, 300])\n"
     ]
    }
   ],
   "source": [
    "# define vocabulary and space vector\n",
    "VOCAB_SIZE = 5000\n",
    "EMBED_DIM = 300\n",
    "\n",
    "embedding = nn.Embedding(VOCAB_SIZE, EMBED_DIM)\n",
    "\n",
    "print(f\"size of the embedding weights: {embedding.weight.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWEMWithEmbeddings(\n",
      "  (embedding): Embedding(5000, 300)\n",
      "  (fc1): Linear(in_features=300, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SWEMWithEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_dim, num_outputs):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.fc1 = nn.Linear(embedding_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=0)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SWEMWithEmbeddings(\n",
    "    vocab_size=5000,\n",
    "    embedding_size=300,\n",
    "    hidden_dim=64,\n",
    "    num_outputs=1,\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs shape: torch.Size([5, 1, 300])\n"
     ]
    }
   ],
   "source": [
    "mb = 1\n",
    "x_dim = 300 \n",
    "sentence = [\"recurrent\", \"neural\", \"networks\", \"are\", \"great\"]\n",
    "\n",
    "xs = []\n",
    "for word in sentence:\n",
    "    xs.append(torch.tensor(normalized_embeddings[index[word]]).view(1, x_dim))\n",
    "    \n",
    "xs = torch.stack(xs, dim=0)\n",
    "print(\"xs shape: {}\".format(xs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 128]) torch.Size([128]) torch.Size([128, 128]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "h_dim = 128\n",
    "\n",
    "# For projecting the input\n",
    "Wx = torch.randn(x_dim, h_dim)/np.sqrt(x_dim)\n",
    "Wx.requires_grad_()\n",
    "bx = torch.zeros(h_dim, requires_grad=True)\n",
    "\n",
    "# For projecting the previous state\n",
    "Wh = torch.randn(h_dim, h_dim)/np.sqrt(h_dim)\n",
    "Wh.requires_grad_()\n",
    "bh = torch.zeros(h_dim, requires_grad=True)\n",
    "\n",
    "print(Wx.shape, bx.shape, Wh.shape, bh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden state h1 dimensions: torch.Size([1, 128])\n",
      "Hidden state h2 dimensions: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "def RNN_step(x, h):\n",
    "    return torch.tanh((torch.matmul(x, Wx) + bx) + (torch.matmul(h, Wh) + bh))\n",
    "\n",
    "\n",
    "# Word embedding for first word\n",
    "x1 = xs[0, :, :]\n",
    "\n",
    "# Initialize hidden state to 0\n",
    "h0 = torch.zeros([mb, h_dim])\n",
    "\n",
    "# Forward pass of one RNN step for time step t=1\n",
    "h1 = RNN_step(x1, h0)\n",
    "print(\"Hidden state h1 dimensions: {0}\".format(h1.shape))\n",
    "\n",
    "# Word embedding for second word\n",
    "x2 = xs[1, :, :]\n",
    "\n",
    "# Forward pass of one RNN step for time step t=2\n",
    "h2 = RNN_step(x2, h1)\n",
    "print(\"Hidden state h2 dimensions: {0}\".format(h2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN parameter shapes: [torch.Size([128, 300]), torch.Size([128, 128]), torch.Size([128]), torch.Size([128])]\n",
      "Hidden states shape: torch.Size([5, 1, 128])\n",
      "Final hidden state shape: torch.Size([1, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "rnn = nn.RNN(x_dim, h_dim)\n",
    "print(\"RNN parameter shapes: {}\".format([p.shape for p in rnn.parameters()]))\n",
    "\n",
    "hs, h_T = rnn(xs)\n",
    "\n",
    "print(\"Hidden states shape: {}\".format(hs.shape))\n",
    "print(\"Final hidden state shape: {}\".format(h_T.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM parameters: [torch.Size([512, 300]), torch.Size([512, 128]), torch.Size([512]), torch.Size([512])]\n",
      "GRU parameters: [torch.Size([384, 300]), torch.Size([384, 128]), torch.Size([384]), torch.Size([384])]\n"
     ]
    }
   ],
   "source": [
    "lstm = nn.LSTM(x_dim, h_dim)\n",
    "print(\"LSTM parameters: {}\".format([p.shape for p in lstm.parameters()]))\n",
    "\n",
    "gru = nn.GRU(x_dim, h_dim)\n",
    "print(\"GRU parameters: {}\".format([p.shape for p in gru.parameters()]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
